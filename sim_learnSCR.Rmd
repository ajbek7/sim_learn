---
title: "sim_learn"
author: "AJB"
date: "2023-10-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Github

```{r}

library(usethis)
usethis::use_github()
```



# Causal Diagram

```{r}
#install.packages("dagitty")
library(dagitty)

```


```{r}

simdi <- dagitty("dag {
                Y <- W -> X
                X -> Y
                e -> X
                }")

# Manually set the coordinates for each node
# The coordinates function expects a named list of x,y pairs

coordinates(simdi) <- list(
  x = c(X = 0, Y = 1, e = -1, W = 1),
  y = c(X = 0, Y = 0, e = 0, W = -1)
)
# Now plot the DAG with the specified coordinates
# Overlay bold labels

plot(simdi, textsize = 5)

```
# This is a test of a pull
# Creating Data

## installing tidyverse

```{r}
#install.packages("tidyverse")

```

## Creating data that follows Causal Diagram

```{r}

library(tidyverse)

# W is continuous; X is binary; Y is continuous; e will be random noise

# If we want the random data to be the same every time, set a seed
set.seed(1000)

# tibble() creates a tibble; it's like a data.frame
# We must pick a number of observations, here 200

d <- tibble(W = runif(200, 0, .1)) %>% # only go from 0 to 0.1
  mutate(X = runif(200) < .2 + W) %>% # runif(200) is what creates e
  mutate(Y = 3*X + W + rnorm(200)) # True effect of X on Y is 3



```

We can go one step further. Not only do we want to randomly create this data, we are going to want to do so a bunch of times as we repeatedly run our simulation. Any time we have code we want to run a bunch of times, it’s often a good idea to put it in its own function, which we can repeatedly call. We’ll also give this function an argument that lets us pick different sample sizes.

```{r}
# library(tidyverse)
# Make sure the seed goes OUTSIDE the function. It makes the random 
# data the same every time, but we want DIFFERENT results each time 
# we run it (but the same set of different results, thus the seed)
set.seed(1000)

# Make a function with the function() function. The "N = 200" argument 
# gives it an argument N that we'll use for sample size. The "=200" sets
# the default sample size to 200

create_data <- function(N = 200){
  d <- tibble(W = runif(N, 0, 0.1)) %>%
    mutate(X = runif(N) < 0.2 + W) %>%
    mutate(Y = 3*X + W + rnorm(N))
  
  # Use return() to send our created data back
  return(d)
}

# Run our function!

d550 <- create_data(550)



```


# Creating Panel Data

```{r}
# library(tidyverse)
set.seed(1000)

# N for number of individuals, T for time periods

create_panel_data <- function(N = 200, T = 10){
  
  # Create individual IDs with the crossing()
  # function, which makes every combination of two vectors
  # (if you want some to be incomplete, drop them later)
  
  panel_data <- crossing(ID = 1:N, t = 1:T) %>%
    
    # And individual/time-varying data
    
    # (n() means "the number of rows in this data"):
    
    mutate(W1 = runif(n(), 0, 0.1))
  
  
    # Now an individual-specific characteristic
  
    indiv_data <- tibble(ID = 1:N, W2 = rnorm(N))
    
    # Join them
    
    panel_data <- panel_data %>%
      full_join(indiv_data, by = 'ID') %>%
      
      # Create X, caused by W1 and W2
      
      mutate(X = 2*W1 + 1.5*W2 + rnorm(n())) %>%
      
      # And create Y. The true effect of X on Y is 3
      
      # But W1 and W2 have causal effects too
      
      mutate(Y = 3*X + W1 - 2*W2 + rnorm(n()))
    
    
    return(panel_data)
    
    
}
```


```{r}
# take it for a spin

d_pan <- create_panel_data(100, 5)


```


# Creating Heteroskedasticity

```{r}

# libray(tidyverse)

set.seed(1000)

create_het_data <- function(N = 200){
  
  d <- tibble(X = runif(N)) %>%
    
    # Let the standard deviation of the error
    # be related to X. Heteroskedasticity!
    
    mutate(Y = 3*X + rnorm(N, sd = X*5))
  
  
    return(d)
}



```

```{r}
# runnin it

d_het <- create_het_data(500)



```

```{r}
# visualizing a class "fan" shape we've created

plot(d_het)



```


# Simulating Clustering

First, we need something to cluster at: some group-identifier.

Then, we can create clustered standard errors by creating or randomly generating a single "group effect" 
that can be shared by the group itself, adding on individual noise.

In other words, generate an individual-level error term, then an individual/time-level error term,
and add them together in some fashion to get clustered standard errors.

```{r}

library(tidyverse)

set.seed(1000)

# N for number of individuals, T for time periods

create_clus_data <- function(N = 200, T = 10){
  
  # We're going to create errors clustered at the
  
  # ID level. So we can follow our steps from making panel data
  
  panel_data <- crossing(ID = 1:N, t = 1:T) %>%
    
    # Individual/time-varying data
    
    mutate(W = runif(n(), 0, 0.1))
  
  # Now an individual-specific error cluster
  
  indiv_data <- tibble(ID = 1:N,
                       C = rnorm(N))
  
  # Join them
  
  panel_data <- panel_data %>%
    full_join(indiv_data, by = 'ID') %>%
    
    # Create X, caused by W1 and W2
    
    mutate(X = 2*W + rnorm(n())) %>%
    
    # The error term has two components: the individual
    # cluster C, and the individual-and-time-varying element
    
    mutate(Y = 3*X + (C + rnorm(n())))
  
  return(panel_data)
  
}


d_cluster <- create_clus_data(100, 5)
```


# Function to generate panel data and estimate model


```{r}

# library(tidyverse)

set.seed(1000)

# A function for estimation

est_model <- function(N, T) {
  
  # Get our data. This uses create_clus_data from earlier
  
  d <- create_clus_data(N, T)
  
  # Run a model that should be unbiased
  
  # if clustered errors themselves don't bias us!
  
  m <- lm(Y ~ X + W, data = d)
  
  # Get the coefficient on X, which should be true value 3 on average
  
  x_coef <- coef(m)['X']
  
  return(x_coef)
  
}

# Run our model

est_model(200, 5)


```


# Creating Iteration

```{r}
# library(tidyverse)

library(purrr)

set.seed(1000)

# Estimate our model 1000 times (from 1 to 1000)

estimates <- 1:1000 %>%
  
  # Run the est_model function each time
  
  map_dbl(function(x) est_model(N = 200, T = 5))

# There are many map functions in purrr. Since est_model outputs a number
# (a "double"), we can use map_dbl and get a vector of estimates




```


# Coding Agus's estimator 

```{r}

# library(tidyverse); library(purrr)

library(broom)

# Data creation function. Let's make the function more flexible
# so we can choose our own true effect!

create_data <- function(N, true){
  
  d <- tibble(X = rnorm(N)) %>%
    
    mutate(Y = true*X + rnorm(n()))
  
    return(d)
}


# Estimation function. keep is the portion of data in each tail to keep.

# So 0.2 would keep the bottom and top 20% of X

est_model <- function(N, keep, true){
  
  d <- create_data(N, true)
  
  # Agus' estimator!
  
  m <- lm(Y~X, data = d %>%
            filter(X <= quantile(X, keep) | X >= quantile(X, (1-keep))))
  
  # Return coefficient and standard error as two elements of a list
  
  ret <- tidy(m)
  
  return(list('coef' = ret$estimate[2],
          'se' = ret$std.error[2]))
  
}


# Run 1000 simulations. Use map_df to stack all the results

# together in a data frame

results <- 1:1000 %>%
  
  map_df(function(x)est_model(N = 1000,
                              keep = 0.2,
                              true = 2))


mean(results$coef); sd(results$coef); mean(results$se)
```


# Comparing Estimators

```{r}
#install.packages("vtable")
library(tidyverse); library(purrr); library(broom); library(vtable)

set.seed(1000)

# Estimation function. keep is the portion of data in each tail
# to keep. So 0.2 would keep the bottom and top 20% of X

est_model <- function(N, keep, true){
  
  d <- create_data(N, true)
  
  # Regular estimator!
  
  m1 <- lm(Y~X, data = d)
  
  
  # Agus's estimator!
  
  m2 <- lm(Y ~ X, data = d %>%
             filter(X <= quantile(X, keep) | X >= quantile(X, (1-keep))))
  
  
  # Return coefficients as a list
  
  return(list('coef_reg' = coef(m1)[2],
              'coef_agus' = coef(m2)[2]))
  
}


# Run 1000 simulations. Use map_df to stack all the

# results together in a data frame

results <- 1:1000 %>%
  
  map_df(function(x) est_model(N = 1000,
                               keep = 0.2, true = 2))



sumtable(results)


```

# Coding Back Doors


```{r}

# library(tidyverse); library(purrr)

set.seed(1000)

# Have settings for strength of W -> X and for W -> Y

# These are relative to the standard deviation of 

# the random components of X and Y, which are 1 each

# (rnorm() defaults to a standard deviation of 1)

create_data <- function(N, effectWX, effectWY){
  
  d <- tibble(W = rnorm(N)) %>%
    
    mutate(X = effectWX*W + rnorm(N)) %>%
    
    # True effect is 5
    
    mutate(Y = 5*X + effectWY*W + rnorm(N))
  
  return(d)
  
  
}


# Our estimation function

est_model <- function(N, effectWX, effectWY){
  
  d <- create_data(N, effectWX, effectWY)
  
  
  # Biased estimator - no W control!
  
  # But how bad is it?
  
  m <- lm(Y~X, data = d)

  
  return(coef(m)['X'])  
  
  
}


# Iteration function! We'll add an option iters for number of iterations

iterate <- function(N, effectWX, effectWY, iters){
  
  results <- 1:iters %>%
    
    map_dbl(function(x){
      
      # Let's add something that lets us keep track
      
      # of how much progress we've made. Print every 100th iteration
      
      if(x %% 100 == 0){print(x)}
      
      # Run our model and return the result
      
      return(est_model(N, effectWX, effectWY))
      
    })
  
  # We want to know *how biased* it is, so compare to true-effect 5
  
  return(mean(results) - 5)
  
  
}

# Now try different settings to see how bias changes!

# Here we'll use a small number of iterations (200) to 

# speed things up, but in general bigger is better

# N = 2000, iters = 200

```

## effectWX=0, effectWY=0

```{r}
# This should produce unbiased estimate

iterate(2000, 0, 0, 200)


# As expected the coefficient is 0.00
```


## effectWX=0, effectWY=1

```{r}

# Because this is still not a backdoor, the coefficient should be unbiased

iterate(2000, 0, 1, 200)

# As expected the coefficient is 0.00


```


## effectWX=1, effectWY=1

```{r}

# Here I expect biased results, though the effect is just multiplication by a 1

iterate(2000, 1, 1, 200)

# Indeed, the coefficient is 0.499

```

## effectWX=0.1, effectWY=0.1

```{r}

# Here I expect the effect to be more modest

iterate(2000, 0.1, 0.1, 200)

# Indeed the coef is nearly 0: 0.01

```


## effectWX=0.5, effectWY=0.1

```{r}

# Does it make a difference whether the effect is stronger on X or Y?

# I predict that it matters

iterate(2000, 0.5, 0.1, 200)

# The coef is 0.04

```


## effectXW=0.1, effectWY=0.5

```{r}

# Switching it

iterate(2000, 0.1, 0.5, 200)

# No, it doesn't matter much as long as there is a back door
# coef here is 0.05
```











